

## The approach used for comparison
OpenAI has created [humanEval](https://arxiv.org/abs/2107.03374) benchmark to evaluate correctness of code generated by LLMs trained on code.
It consists of definition of pass@k metric and a set of 164 programming problems with varying difficulty levels.

Pass@k can also be interpreted as the result of evaluating the best out of k samples, where the best sample is picked by an oracle with prior knowledge of the unit tests.



In particular, for a 679M parameter model, the optimal temperature for pass@1 is T
∗ = 0.2 and the optimal temperature for pass@100 is T
∗ = 0.8. With these temperatures,
we find that pass@1 and pass@100 scale smoothly as a
function of model size (Figure 6).



From
a practical perspective, we are also interested in the setting where we must select a single sample from k samples
without having access to an oracle. For instance, when the
model is used as an autocomplete tool where a user provides
a prompt, we do not have unit tests, but would like to return
only a single completion to the user for evaluation so as to
not overwhelm them.

 

There are 2 proposed ways to work 

I prefer pass@1 with low temperature to get more reliable code (as reliable as training data) due to the lesser probability or performance/memory degradation of security vulnerability introduced by randomness.


pass@1 with TopP 0.95 + temperature of 0.2 as recommended for pass@1 by OpenAI in HumanEval paper 


| Model | Gemini CodeAssist | GitHub Copilot | Amazon Q (CodeWhisperer) | Cursor | Qodo (Codium) |
| -- | -- | -- | -- | -- | -- |
