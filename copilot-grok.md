
| Model | GitHub Copilot (gpt-4-turbo) | Grok 2-latest (grok-2-1212) | Grok 3- latest (grok-3-xxxx)|
| -- | -- | -- | -- | 
| humanEval pass@1, T=0.2, TopP=0.95 | 89% | 85% | ?? | 
| MBPP pass@1, T=0.2, TopP=0.95 | ?? | ?? | ?? | 


## The approach used for comparison

### HumanEval
OpenAI has created [humanEval](https://arxiv.org/abs/2107.03374) benchmark to evaluate correctness of code generated by LLMs trained on code.
It consists of definition of pass@k metric and a set of 164 programming problems with varying difficulty levels with unit tests.

Pass@k can be interpreted as the result of evaluating the best out of k samples, where the best sample is picked by an oracle with prior knowledge of the unit tests.

They determined that when evaluating pass@k, it is important to optimize sampling temperature for the particular value of k. 
The higher temperatures are optimal for larger k, because the resulting set of samples has higher diversity, and the metric rewards only whether the model generates any correct solution (see Pass@K vs K, temperature).
And optimal temperature for pass@1 is T∗ = 0.2 and the optimal temperature for pass@100 is T∗ = 0.8.

While pass@100 with the high temperature of 0.8 leads to better results (pass@k rewars only whether the model generates any correct solution) I prefer pass@1 with low temperature of 0.2 to get more reliable code (as reliable as training data) due to the lesser probability or performance/memory degradation or security vulnerability introduced by randomness.

### MBPP

