
| Model | GitHub Copilot (gpt-4-turbo) | Google Code Assist | Grok 2-latest (grok-2-1212) | Grok 3-latest (grok-3-xxxx)|
| -- | -- | -- | -- | -- |
| humanEval pass@1, T=0.2, TopP=0.95 | 89% | ?? | 85% | ?? | 
| MBPP pass@1, T=0.2, TopP=0.95 | ?? | ?? | ?? | ?? | 


## The approach used for comparison
For comparison, I use the pass@1 metric with a low temperature of 0.2 and datasets from HumanEval and MBPP benchmarks. 
I deliberately don't use APPS dataset as it seems we are far yet from good pass rates even for introductory-level problems, see https://arxiv.org/pdf/2105.09938.


### HumanEval
OpenAI has created [humanEval](https://arxiv.org/abs/2107.03374) benchmark to evaluate correctness of code generated by LLMs trained on code.
It consists of definition of pass@k metric and a set of 164 programming problems with varying difficulty levels with unit tests.

Pass@k can be interpreted as the result of evaluating the best out of k samples, where the best sample is picked by an oracle with prior knowledge of the unit tests.

They determined that when evaluating pass@k, it is important to optimize sampling temperature for the particular value of k. 
The higher temperatures are optimal for larger k, because the resulting set of samples has higher diversity, and the metric rewards only whether the model generates any correct solution (see Pass@K vs K, temperature).
And optimal temperature for pass@1 is T = 0.2 and the optimal temperature for pass@100 is T = 0.8.

While pass@100 with the high temperature of 0.8 leads to better results (pass@k rewars only whether the model generates any correct solution) I prefer pass@1 with low temperature of 0.2 to get more reliable code (as reliable as training data) due to the lesser probability or performance/memory degradation or security vulnerability introduced by randomness.

### MBPP (Mostly Basic Python Programming )
The [MBPP](https://arxiv.org/pdf/2108.07732v1) dataset created by Google contains 974 programming tasks, designed to be solvable by entry-level programmers.
Each problem consists of a task description, code solution, and 3 unit tests.

The dataset is similar to HumanEval but more closely mimics test docstrings of professional software with 3 I/O examples, written as assert statements.


### Related
1. Gemini model architecture and HumanEval testing (May 2024): https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf
2. Measuring Coding Challenge Competence With APPS https://arxiv.org/pdf/2105.09938
